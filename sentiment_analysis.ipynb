{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\zeugn\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package sentence_polarity to\n",
      "[nltk_data]     C:\\Users\\zeugn\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package sentence_polarity is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Import necessary packages\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import sentence_polarity\n",
    "nltk.download('sentence_polarity')\n",
    "import random\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import re\n",
    "import collections\n",
    "from nltk.metrics import *\n",
    "from nltk.classify import MaxentClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read text file that contains amazon product reviews\n",
    "file = open('baby_text.txt', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of reviewText sentences\n",
    "TextList = []\n",
    "YearList = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reg ex to extract the review text and the year\n",
    "review_pattern = r'''(?x) reviewText[:](.+) '''\n",
    "\n",
    "year_pattern = r''' (?x) reviewTime[:](.+)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loop through the text file and extract the year as well as the review text\n",
    "#Clean strings\n",
    "for line in file:\n",
    "    sentlist = []\n",
    "    reviewyear_tokens = nltk.regexp_tokenize(line, year_pattern)\n",
    "    reviewtext_tokens = nltk.regexp_tokenize(line, review_pattern)\n",
    "        \n",
    "    if(len(reviewtext_tokens)>0):\n",
    "        current_review = reviewtext_tokens[0].strip(\"\\n\") \n",
    "        TextList.append(current_review)\n",
    "    if(len(reviewyear_tokens)>0):\n",
    "        current_year = reviewyear_tokens[0].split(',')[1].strip('\\n').strip(' ')\n",
    "        YearList.append(current_year)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select a year to filter off and append review text      \n",
    "final = []\n",
    "for i in range(0, len(YearList)):\n",
    "    if(YearList[i]=='2014'):\n",
    "        final.append(TextList[i])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of reviews 35691\n"
     ]
    }
   ],
   "source": [
    "print(\"number of reviews\",len(final))\n",
    "final= final[:10000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "print(len(final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentence tokenization on the review text\n",
    "final_sentlist = []\n",
    "for review in final:\n",
    "    curr_sent_list = nltk.sent_tokenize(review)\n",
    "    for sent in curr_sent_list:\n",
    "        final_sentlist.append(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Helps me know exactly how my babies day has gone with my mother in law watching him while I go to work.',\n",
       " 'It also has a section for her to write notes and let me know anything she may need.',\n",
       " \"I couldn't be happier with this book.\",\n",
       " 'I wanted an alternative to printing out daily log sheets for the nanny to fill out, and this has worked out great!',\n",
       " \"I'm no longer searching my daughter's bag for a crumpled piece of paper each day.\"]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_sentlist[:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_pattern2 = r''' (?x)\n",
    "    [][.,;\"'?!():-_%']    \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "textList = []\n",
    "for sent in final_sentlist:\n",
    "    sent = sent.lower()\n",
    "    words = nltk.word_tokenize(sent)\n",
    "    if(len(words)==1):\n",
    "        if(len(nltk.regexp_tokenize(words[0], text_pattern2))<0):\n",
    "            textList.append(sent)\n",
    "    else:\n",
    "        textList.append(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43821"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(textList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_sentlist = textList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43821"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_sentlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "bag_of_words = sentence_polarity.sents()\n",
    "sentlabels = [(sent, label) for label in sentence_polarity.categories()\n",
    "                     for sent in sentence_polarity.sents(categories=label)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(sentlabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "allsents = [word for (sent, label) in sentlabels for word in sent]\n",
    "allsents_freq = nltk.FreqDist(allsents)\n",
    "freqwords = allsents_freq.most_common(2000)\n",
    "freqword_features = [word for (word,freq) in freqwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_features(sentence, words):\n",
    "    unique_words = set(sentence)\n",
    "    features = {}\n",
    "    for word in words:\n",
    "        features['contains({})'.format(word)] = (word in unique_words)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "featureset = [(document_features(sent, freqword_features), label) for (sent, label) in sentlabels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_featureset, test_featureset = featureset[1000:], featureset[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = nltk.NaiveBayesClassifier.train(train_featureset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Accuracy 0.76\n"
     ]
    }
   ],
   "source": [
    "print(\"Baseline Accuracy\",nltk.classify.accuracy(classifier, test_featureset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_accuracy_measures(actual_set, input_classifier):\n",
    "    reference_sets = collections.defaultdict(set)\n",
    "    predicted_sets = collections.defaultdict(set)\n",
    "    for i, (feats, label) in enumerate(actual_set):\n",
    "        reference_sets[label].add(i)\n",
    "        predicted = input_classifier.classify(feats)\n",
    "        predicted_sets[predicted].add(i)\n",
    "    print('Positive precision:', precision(reference_sets['pos'], predicted_sets['pos']))\n",
    "    print('Positive recall:', recall(reference_sets['pos'], predicted_sets['pos']))\n",
    "    print('Positive F-measure:', f_measure(reference_sets['pos'], predicted_sets['pos']))\n",
    "    print('Negative precision:', precision(reference_sets['neg'], predicted_sets['neg']))\n",
    "    print ('Negative recall:', recall(reference_sets['neg'], predicted_sets['neg']))\n",
    "    print ('Negative F-measure:', f_measure(reference_sets['neg'], predicted_sets['neg'])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_sentencelist = []\n",
    "neg_sentencelist = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sent in final_sentlist:\n",
    "    if(classifier.classify(document_features(nltk.word_tokenize(sent),freqword_features)) == 'pos'):\n",
    "        pos_sentencelist.append(sent)\n",
    "    elif(classifier.classify(document_features(nltk.word_tokenize(sent),freqword_features)) == 'neg'):\n",
    "        neg_sentencelist.append(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Accuracy 0.76\n"
     ]
    }
   ],
   "source": [
    "print(\"Baseline Accuracy\",nltk.classify.accuracy(classifier, test_featureset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive precision: 0.8034449760765551\n",
      "Positive recall: 0.7874695179140874\n",
      "Positive F-measure: 0.7953770367563471\n",
      "Negative precision: 0.7916130218870701\n",
      "Negative recall: 0.8073532170324517\n",
      "Negative F-measure: 0.799405646359584\n"
     ]
    }
   ],
   "source": [
    "display_accuracy_measures(featureset, classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive reviews  19842\n",
      "negative reviews  47957\n"
     ]
    }
   ],
   "source": [
    "print(\"positive reviews \",len(pos_sentencelist))\n",
    "print(\"negative reviews \",len(neg_sentencelist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['highly recommended as a perfect way to keep track and organize information in one easy to travel with place!', 'my son loves to chew on things and is still awaiting his first tooth.', 'it is a great toy for them to chew and play with, but it is a bit harder for smaller babies to chew on.', 'my daughter finds this very interesting especially when it gently vibrates and massages her gums.', 'this is a great teether.']\n"
     ]
    }
   ],
   "source": [
    "print(pos_sentencelist[:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this is great for basics, but i wish the space to write things in was bigger.', 'i wanted to love this, but it was pretty expensive for only a few months worth of calendar pages.', 'i ended up buying a regular weekly planner - 55% off!', '- the planner - that is 8 1/2 x 11 and has all seven days on the right page and the left page has room to write a to do list and goals.', 'this planner was cute, just not what i wanted.', 'has columns for all the info i need at a glance when i get home from work.']\n"
     ]
    }
   ],
   "source": [
    "print(neg_sentencelist[6:12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "negationwords = ['no', 'not', 'never', 'none', 'nowhere', 'nothing', 'rather', 'hardly', 'scarcely', 'rarely', 'neither', 'nor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43821"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_sentlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NOT_features(document, word_features, negationwords):\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains({})'.format(word)] = False\n",
    "        features['contains(NOT{})'.format(word)] = False\n",
    "    # Parse the document words in order\n",
    "    for i in range(0, len(document)):\n",
    "        word = document[i]\n",
    "        if ((i + 1) < len(document)) and ((word in negationwords) or (word.endswith(\"n't\"))):\n",
    "            i += 1\n",
    "            features['contains(NOT{})'.format(document[i])] = (document[i] in word_features)\n",
    "        else:\n",
    "            features['contains({})'.format(word)] = (word in word_features)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtaining the feature set for the sentences \n",
    "feature_set_2 = [(NOT_features(sent, freqword_features, negationwords), label) for (sent, label) in sentlabels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the feature set data into training and test data\n",
    "train_feature_set_2, test_feature_set_2 = feature_set_2[2000:], feature_set_2[:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply NB Classification on the training feature set\n",
    "classifier_2 = nltk.NaiveBayesClassifier.train(train_feature_set_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negation Accuracy 0.769\n"
     ]
    }
   ],
   "source": [
    "#Determine the Accuracy\n",
    "print(\"Negation Accuracy\",nltk.classify.accuracy(classifier_2, test_feature_set_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive precision: 0.9276278001148766\n",
      "Positive recall: 0.9088351153629713\n",
      "Positive F-measure: 0.9181353041500853\n",
      "Negative precision: 0.9106453392167678\n",
      "Negative recall: 0.9290939786156444\n",
      "Negative F-measure: 0.9197771587743733\n"
     ]
    }
   ],
   "source": [
    "#Display the other measures\n",
    "display_accuracy_measures(feature_set_2, classifier_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the positive and negative lists\n",
    "pos_sentence_list2 = []\n",
    "neg_sentence_list2 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sent in final_sentlist:\n",
    "    if(classifier_2.classify(NOT_features(nltk.word_tokenize(sent),freqword_features, negationwords)) == 'pos'):\n",
    "        pos_sentence_list2.append(sent)\n",
    "    elif(classifier_2.classify(NOT_features(nltk.word_tokenize(sent),freqword_features,negationwords)) == 'neg'):\n",
    "        neg_sentence_list2.append(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of positive reviews  13777\n",
      "The number of negative reviews  30044\n"
     ]
    }
   ],
   "source": [
    "#Printing the number of positive and negative reviews\n",
    "print(\"The number of positive reviews \",len(pos_sentence_list2))\n",
    "print(\"The number of negative reviews \",len(neg_sentence_list2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i have used this tracker for all 3 of my children and swear by it!', 'i also used it to jot down other important &#34;mile stone&#34; things that i would have quickly forgotten&#8230;like outings, temperature readings, visits from friends to meet him, calls to make, when we stopped swaddling him, etc.', 'i went back to formally fill out his baby album since i would have forgotten so many of those valuable little tidbits of information & memories.', \"we use this everyday to communicate feeding, medicine and other needs when i don't have time to give my nanny the full rundown.\"]\n"
     ]
    }
   ],
   "source": [
    "print(pos_sentence_list2[6:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"i would have preferred a plastic-type cover, but it's held up well so far.\", 'this is great for basics, but i wish the space to write things in was bigger.', 'i wanted to love this, but it was pretty expensive for only a few months worth of calendar pages.', 'i ended up buying a regular weekly planner - 55% off!']\n"
     ]
    }
   ],
   "source": [
    "print(neg_sentence_list2[6:10])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
